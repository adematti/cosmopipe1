.. _user-scripting:

Scripting
=========

Example
-------

Let's say we want to construct a joint likelihood of two data vectors ``data1`` and ``data2``, with their own models ``model1`` and ``model2``
and a common covariance matrix ``cov``. Our parameter ``.ini`` file would then be:

.. code-block:: ini

  [main]
  modules = like

  [like]
  module_name = cosmopipe.likelihood.likelihood
  module_class = JointGaussianLikelihood
  join = like1 like2
  modules = cov

  [like1]
  module_name = cosmopipe.likelihood.likelihood
  module_class = BaseLikelihood
  modules = data1 model1

  [like2]
  module_name = cosmopipe.likelihood.likelihood
  module_class = BaseLikelihood
  modules = data2 model2

  [data1]
  module_name = cosmopipe.data.data_vector
  ;details about how to get the data vector #1

  [model1]
  ;details about model #1

  [data2]
  module_name = cosmopipe.data.data_vector
  ;details about how to get the data vector #2

  [model2]
  ;details about model #2

  [cov]
  module_name = cosmopipe.data.covariance
  ;details about how to get the common covariance


Here module names (in brackets) can be of any name, except ``[main]`` which is the entry to the pipeline.
The fields ``modules`` provided for :class:`~cosmopipe.pipeline.module.BasePipeline` inherited-modules  (called (sub)pipelines in the following)
list the modules they run.

.. note::

  This file can be provided in yaml format as well.

One can achieve the same thing in Python with (for a theory model called :mod:`~cosmopipe.theory.FlatModel`):

.. code-block:: python

  from cosmopipe.pipeline import BaseModule, BasePipeline, ConfigBlock, SectionBlock
  from cosmopipe.theory import FlatModel
  from cosmopipe.likelihood import BaseLikelihood, JointGaussianLikelihood

  config_block = ConfigBlock(config_fn)
  data1 = BaseModule.from_library(name='data1',options=SectionBlock(config_block,'data1'))
  model1 = FlatModel(name='model1')
  data2 = BaseModule.from_library(name='data2',options=SectionBlock(config_block,'data2'))
  model2 = BaseModule.from_library(name='model2',options=SectionBlock(config_block,'model2'))
  cov = BaseModule.from_library(name='cov',options=SectionBlock(config_block,'cov'))
  like1 = BaseLikelihood(name='like1',modules=[data1,model1])
  like2 = BaseLikelihood(name='like2',modules=[data2,model2])
  like = JointGaussianLikelihood(name='like',join=[like1,like2],modules=[cov])
  pipeline = BasePipeline(modules=[like])

Here ``options`` can be simple dictionaries (:class:`~cosmopipe.pipeline.SectionBlock` simply takes a slice of ``config_block`` for the given section).
You can also modify each module's ``data_block`` at your will.
There is really no more complexity than using Python classes successively, with **cosmopipe** holding all these classes.
So even if you do not like the **cosmopipe** framework, you can still use **cosmopipe** modules very easily in your own code.

In diagrammatic representation (generated by :meth:`~cosmopipe.pipeline.module.BasePipeline.plot_pipeline_graph`):

  .. image:: ../static/pipe3.png

CosmoPipe rules
---------------

The **cosmopipe** framework is agnostic about the actual operations performed by the modules it sets up, executes and cleans up.
This is key to ensuring the base code does not need to be modified when adding a new module.

Similarly, modules are agnostic about the operations performed by other modules.
This is key to ensuring modules do not need to be modified when adding new ones.

Hence, the pipeline integrity is ensured by the user script.
The main difficulty is to ensure that each module takes the input of the preceding module at the relevant entry (``section``, ``name``)
of ``data_block``, the :class:`~cosmopipe.pipeline.block.DataBlock` instance passed to all modules (see :ref:`user-framework`).

CosmoSIS implements a linear pipeline: all modules form a single chain.
Instead, we allow for a tree structure, which is explored depth-first, left to right.
Both approaches would be fully equivalent if the ``data_block`` were a global variable for all modules.
Instead, contrary to CosmoSIS, each (sub)pipeline creates (at initialisation only) a (shallow!) copy of the ``data_block`` to be passed to its modules.

.. note::

  In the example above, ``[model2]`` does not know anything about ``[model1]`` products. If one wanted to add a common calculation beforehand
  (e.g. linear power spectrum), it would be added at the head of the ``modules`` list of ``[main]``
  (not of ``[like]`` because of the peculiar structure of :class:`~cosmopipe.likelihood.likelihood.JointGaussianLikelihood` - its ``modules`` being run *after* ``join``).

Hence, any change made these modules to the ``data_block`` are local (effective within the (sub)pipeline), which we think is the most commmon expected behaviour.
Therefore, a precomputation performed ahead of this (sub)pipeline, saved into ``data_block[section,name]`` will not be erased by the
modules of this (sub)pipeline even if they write in the same entry of ``data_block``.
This allows modules to *update* (for them) previous entries in ``data_block`` and hence to keep a short list of entries (``section``, ``name``) in use.
Then, most of the links between module input and output entries is encoded in the pipeline structure itself.
We think it also makes the pipeline structure more readable.
Yet, this may not be sufficient in some corner cases; we may e.g. want to save the result of a given operation (e.g. derived parameter)
performed at some position in the tree. This is made possible by using the keyword ``copy`` in any module section of the configuration file/dictionary::

  copy = section1.name1,section2.name2

will (shallow!) copy the element from entry (``section1``, ``name1``) to entry (``section2``, ``name2``).
There are three global (i.e. shared by all modules whatever their depth) sections: 'globals', 'parameters' (which contains potentially varying parameters)
and 'likelihood' (which containts 'loglkl', the log-likelihood evaluation). So taking ``section2 = 'globals'`` will make the element accessible anywhere
in the pipeline.

To summarise:
  - we allow for a tree-like structure
  - any change to ``data_block`` is local within a given (sub)pipeline
  - the sections where changes are globals (effective for the whole pipeline) are 'globals', 'parameters', 'likelihoods'
  - if necessary, any entry of ``data_block`` can be moved anywhere (including the 'global' sections) with the keyword ``copy`` in the configuration file/dictionary
  - ``config_block`` is always global.

.. note::

  Our framework is therefore a generalisation of the CosmoSIS structure.
  Therefore, one can always stick to the CosmoSIS structure if more intuitive.

.. note::

  It is left to the user not to generate loops in their pipeline.

.. note::

  The ``execute`` function of each module is called at *each step*. This meaning depends on the context.
  If your (sub)pipeline performs an MCMC sampling, for example, then the top ``execute`` of this pipeline will be called at each MCMC step.
  But we can imagine that we loop on different data vectors instead. In this case, ``execute`` will be called for each of this vector.
  For example, we want to estimate the power spectrum of a mock catalogue, then perform cosmologic inference.
  Our top base pipeline would run the modules corresponding to the power spectrum estimator, and the sampler.
  One could also imagine generating mocks before estimating their power spectrum, etc.
